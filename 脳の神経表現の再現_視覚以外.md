# 脳の神経表現の再現（運動野・場所受容野）
視覚以外について紹介。

- [運動野](#運動野)
- [場所受容野](#場所受容野)

## 運動野
### D. Sussillo, M. Churchland, M. Kaufman, K. Shenoy. "A neural network that finds a naturalistic solution for the production of muscle activity". *Nat. Neurosci.* **18**(7), 1025–1033 (2015). ([PubMed](https://www.ncbi.nlm.nih.gov/pubmed/26075643))
RNN(recurrent neural network)にサルの筋電活動(electromyographic; EMG)を出力するよう学習させると、RNNのユニットは運動野のニューロンのダイナミクスに似た発火パターンを生み出したという論文。出力が筋電活動なのに中間にニューロンの活動が現れたのが面白い。  

まず、サルにさせたタスクだが、遅延到達課題(delayed reach task)という。画面にタッチすると迷路とゴールが出現する。サルは触れた点からゴールまでの道筋を決めて画面上をなぞればよいわけだが、実際に行動を起こすまでに"待て"をさせられる。実行キュー(Go cue)が出ればタスクを実行、すなわち腕の筋肉を動かして指をゴールまで持っていく(Figure 1a)。タスクは全部で27条件。神経活動は1次運動野(M1)および運動前野背側部(dorsal PMd)から記録された。(Figure 1b)は1つのニューロンの活動の例(27条件における活動が重ねられている)。  

RNNの教師信号はタスク実行時の筋電活動(EMG)としている。入力はGo Cueが出されるまでの準備期間の神経活動をPCAで次元削減した6入力 + 待機信号1入力の計7入力である(Figure 1c)。RNNは正則化をかけたモデル(regularized model)と正則化をかけなかったモデル(complicated model)の2つのモデルを作成した。  

(Figure 2)は学習後の結果を運動野のニューロンの活動と比較したものである。(a)が運動野のニューロン、(b)がregularized modelのユニットの活動、(c)がcomplicated modelのユニットの活動である。正則化をかけたregularized modelでは運動野のニューロンの活動に似た発火パターンを持つユニットが見つかった。  

面白いことに、正則化をかけなかったモデルでは似たようなパターンが出現しなかった（c）。Discussionでも触れられているが、正則化をかけて学習することで実際のニューロンの活動に近くなったというのは視覚でも（加えて下記の場所受容野でも）起こった現象である。脳がいかに学習において正則化をかけているかということについての答えはまだないが、重要な性質であることに間違いない。  

さて、先ほどの図は運動野とRNNの1つのニューロン間での比較だった。それではニューロン全体の活動が似ていることを示すにはどうすればよいだろうか。視覚の場合、Yaminsは線形再構成法を用いたが、ここでは **正準相関分析**(canonical correlation analysis; CCorA)を用いている。正準相関分析は2つのデータに共通するパターンを見つけ出す手法である。(Figure 4)は正準相関分析を行った後の第1,2,5,9,10正準変数(canonical variables; CV)。(a)が運動野、(b)がregularized modelである。図中のrは正準相関を表す。また、complicated modelについても正準相関分析を行っており、(c)が運動野、(d)がcomplicated modelである。正準相関の値の平均値はregularized modelの方がcomplicated modelよりも大きいので、全体的な発火パターンとしてもregularized modelの方がcomplicated modelよりも良い当てはまりであった。  

こうして部分・全体の両面において、regularized modelは筋電活動のみを教師信号として運動野のダイナミクスを生み出したのである。他にはダイナミクスの解析もしており、興味深い論文である。  

## 場所受容野
弊研究室で2018年に最も盛り上がったニューラルネットワークの論文。論文は2つ。主張は同じで、自己運動の速度を積分して自己位置を推定させるタスクをRNNに学習させると、中間層にグリッド細胞のような発火パターンが生まれたというもの。  
### C. Cueva, X. Wei. "Emergence of grid-like representations by training recurrent neural networks to perform spatial localization". *ICLR.* (2018). ([arxiv](https://arxiv.org/abs/1803.07770))
グリッド様の場所受容野がRNN(recurrent neural network)で形成されることを示す最初の論文。入力は並進速度と頭の方向、出力は自己位置(x, y)である(Figure 1b)。  

学習後、格子状(grid-like)、 帯状(band-like)、 境界(border-like)の場所受容野が獲得された(Figure 2)。さらに先に境界に反応する細胞(border cells)が現れてから、グリッド細胞(grid cells)が生じる、という発達過程も一致することも示された(Figure 4)。

### A. Banino, et al. "Vector-based navigation using grid-like representations in artificial agents". *Nat.* **557**(7705), 429–433 (2018). ([pdf](https://deepmind.com/documents/201/Vector-based%20Navigation%20using%20Grid-like%20Representations%20in%20Artificial%20Agents.pdf))
上の論文よりも出たのは少し遅いが、ほぼ同時期。強化学習のタスクで良い成績を出したことが注目されていたが、神経科学的にこの論文の注目すべき点はそこではない。  

タスクは(Cueva et al. 2018)とほぼ同じ。ただし、出力が自己位置のスカラー値ではなく、場所細胞(place cells)と、頭方向細胞(head direction cells)の受容野を獲得することとなっている(Figure 1a)。このモデルは3層からなり、再帰層(LSTM)、線形層、出力層から構成される。  

学習後の線形層には格子状の発火パターンを含む、様々な場所受容野が獲得されていた(Figure 1d)。 さらにこのモデルが学習したグリッド細胞の特性は2つの点で脳内のグリッド細胞と一致していた。まず、獲得された格子間隔（空間基底の単位長さともいえる）は生体と同じくおよそ1.5の比率で生じていた(Figure 1e)。次に格子の角度は壁面から約7.5°ずれていた(Figure 1f)。  

グリッド細胞の発火の方向が壁面から少しずれていることは謎であったが、この実験から、そうであることが最適であるということが分かった。つまり、壁面からずれていることで、格子パターンが空間全体を覆うことができ、一様な場所受容野の形成に役立っているといえる。  

もう1つ、この論文を読んで面白いと思ったのは、ニューラルネットワークを用いて脳の仕組みを明らかにすることについてのDeepMindの考えを知ることができたことである。"Supplementary Discussion:Relationship to previous models of grid cells"(pp.71)より一部引用。

> "It is worth noting that our experiments were not designed to provide insights into the development of grid cells in the brain — due to the limitations of the training algorithm used (i.e. backpropagation) in terms of biological plausibiliy. More generally, however, our findings accord with the perspective that the internal representations of individual brain regions such as the entorhinal cortex arise as a consequence of optimizing for specific ethologically important objective functions (e.g. path integration) — providing a parallel to the optimization process in neural networks."
>

日本語でまとめると、「ニューラルネットワークは誤差逆伝搬法などの学習方法の制約により、生物的な発達まで完全にシミュレーションはできない。しかし、脳の神経表現が生物学的に重要な目的関数（経路積分など）の最適化として生じるということは、ニューラルネットワークを通した実験で知ることができる。」ということ。ニューラルネットワークで脳の仕組みを知ることについては否定的な声もあるが、様々な論文を読んだ感想、および実際に研究している感想は、「ニューラルネットワークを用いれば、神経表現の"How(どのように獲得されたか)"は十分に分からなくても"Why(なぜそのような発火パターンなのか)"は知ることができる」というものである。当然、写像として定義できないことに関してはニューラルネットワークで考えることは難しい。しかし、写像で定義できるものに関しては、ニューラルネットワークは脳の仕組みを知るための強力なツールであると思う。  

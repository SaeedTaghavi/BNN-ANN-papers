# 神経科学に基づいたアーキテクチャ
ニューラルネットワークのアーキテクチャの中で神経科学の知見を取り入れたもの。強化学習系はあまり触れていません(World Modelsとかはそこまで神経科学っぽくないので堪忍)。

---
### D. Hassabis, D. Kumaran, C. Summerfield, M. Botvinick. "Neuroscience-Inspired Artificial Intelligence". *Neuron.* **95**(2), 245-258 (2017). ([sciencedirect](https://www.sciencedirect.com/science/article/pii/S0896627317305093))
Hassabisはこう考えているのか、というのが分かって面白かったレビュー論文。神経科学からインスパイアされた人工神経回路の手法の紹介を前半にして、後半では人工神経回路の研究から脳の仕組みを知ることについても触れられている。日本語での詳しい解説は論文タイトルで検索すれば結構でてくる。  

---
PredNetに関する3つの論文。
### W. Lotter, G. Kreiman, D. Cox. "Deep predictive coding networks for video prediction and unsupervised learning". *ICLR.* (2017). ([arxiv](https://arxiv.org/abs/1605.08104))
Predictive Coding Network(**PredNet**)を提案。ConvLSTMを用いて[予測符号化(Predictive Coding)](https://omedstu.jimdo.com/2018/08/17/%E4%BA%88%E6%B8%AC%E7%AC%A6%E5%8F%B7%E5%8C%96-predictive-coding-%E3%81%A8%E3%81%AF%E4%BD%95%E3%81%8B/)のモデルをニューラルネットワークのモデルとして表現した。タスクとしては1つ前の動画のフレームを用いて、次のフレームを予測するというもの。  

階層構造であることと、双方向結合があることから視覚のモデルとして話題になったが、単にカルマンフィルタをニューラルネットワークのモデルとして実装しただけという意見もある。逆にカルマンフィルタがモデルとして優れていたともいえる。  
（参考）以前、Chainer-v4で実装した。　→　[takyamamoto/PredNet_Chainer](https://github.com/takyamamoto/PredNet_Chainer)

### E. Watanabe, A. Kitaoka, K. Sakamoto, M. Yasugi, K. Tanaka. "Illusory Motion Reproduced by Deep Neural Networks Trained for Prediction". *Front. Psychol.* (2018). ([Front. Psychol.](https://www.frontiersin.org/articles/10.3389/fpsyg.2018.00345/full))
生理研からの論文（[生理研のHPにある解説](http://www.nips.ac.jp/release/2018/03/post_362.html)を読むのが一番分かりやすい）。PredNetに一人称視点の動画を入力し、次フレームを予測するよう学習させた後、ヘビの回転錯視画像を入力すると予測画像は入力画像から少し回転していた（Optical Flowにより解析）。ニューラルネットワークにおいて錯視が出現することを示す最初の論文であり、予測符号化の妥当性を支持するものでもある。

### W. Lotter, G. Kreiman, D. Cox. "A neural network trained to predict future video frames mimics critical properties of biological neuronal responses and perception". (2018). ([arxiv](https://arxiv.org/abs/1805.10734))
PredNetの本家の論文。PredNetに様々な視覚タスクを与え、そのときの発火パターンをマカクザルの視覚野の発火パターンと比較したという論文。  

論文中で挙げられているタスクの内の1つとして、2種類の画像について、画像の切り変わりタイミングと切り替わる画像を学習させるというものがある(Sequence Learning Effects)。Errorユニットでは正しく予想できた場合よりも予想できなかった場合の発火の方が大きかった（単に予測誤差が大きいということだが、PredictionユニットやRepresentationユニットでも同じ傾向があった）。また、PredNetの出力を同じタスクをさせたマカクザルのIT野の81個のニューロンの平均発火率と比較すると同じ傾向がみられた。  

視覚野も将来の画像を予測し予測誤差を計算しているというのが元の論文だが、そのことをうまくモデル化できたということだろう（予測誤差の計算自体は中脳のドーパミンニューロンと同じような仕組みなのだろうか）。  

---  
### R. Costa, Y. Assael, B. Shillingford, N. Freitas, T. Vogels. "Cortical microcircuits as gated-recurrent neural networks". *NIPS.* (2017). ([arxiv](https://arxiv.org/abs/1711.02448))
皮質回路はLSTMのようなゲート構造を持つRNNと見なせる、という論文。LSTMのゲートの役割を抑制性介在細胞(basket細胞)が行っていると仮定し、**subLSTM** という差分ゲートを持つLSTMを提案。ゲートは活性関数をシグモイド関数にして、差分(subtractive)にしている。これが"sub"LSTMである理由。Forgetゲートの代わりにメモリ細胞の中身を（学習可能な）一定の割合で減少させるfix-subLSTMも提案している。皮質が全てLSTMだったらPredictive codingも可能な気がする。  

（余談）以前、subLSTMとConvLSTMを組み合わせたsubConvLSTMをPredNetのRepresentation層に組み込んで学習させた。普通のConvLSTMよりも精度は落ちたが、学習はできた(t=11から外挿(extrapolation)、つまり内的な予測のみで次の予測を生み出している)。  
<img src="https://github.com/takyamamoto/BNN-ANN-papers/blob/master/pictures/output.gif" width=60%>  
